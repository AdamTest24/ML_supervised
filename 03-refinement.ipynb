{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dedd049f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Refinement\"\n",
    "teaching: 10\n",
    "exercises: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc39908",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "[**Download Chapter notebook (ipynb)**](03-refinement.ipynb)\n",
    "\n",
    "[<span style=\"color: rgb(255, 0, 0);\">**Mandatory Lesson Feedback Survey**</span>](https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1)\n",
    "\n",
    ":::::::::: checklist\n",
    "## Submissions\n",
    "- [**Lesson Assignment**](#assign)\n",
    "\n",
    "- [**Lesson Forum**](#forum)\n",
    "::::::::::\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How do different evaluation metrics differ?\n",
    "- What techniques are used to improve on chance prediction?\n",
    "- What are the limitations of a confusion matrix?\n",
    "- How can normalisation and hyperparameter tuning help to improve the results?\n",
    "- How could test data leakage be avoided?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "- Introducing different types of metrics for model evaluation.\n",
    "- Understanding the permutation score.\n",
    "- Illustrating model evaluation using the confusion matrix.\n",
    "- working with normalisation and hyperparameter tuning.\n",
    "- The concept of progressive adjustment.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "<p align = \"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vo9eBk9P9rk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "<br>\n",
    "<p align = \"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JJ_5Dc1Tcg4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "### **Import functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f1d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mgrid, linspace, c_, arange, mean, array\n",
    "from numpy.random import uniform, seed\n",
    "from sklearn.datasets import make_circles\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.pyplot import subplots, axes, scatter, xticks, show\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "RANDOM_STATE = 111\n",
    "\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Random Forest)': AdaBoostClassifier(RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Extra Tree)': AdaBoostClassifier(ExtraTreesClassifier(random_state=RANDOM_STATE)),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'SVC (RBF)': SVC(random_state=RANDOM_STATE),\n",
    "    'SVC (Linear)': LinearSVC(random_state=RANDOM_STATE),\n",
    "    'Multi-layer Perceptron': MLPClassifier(max_iter=5000, random_state=RANDOM_STATE)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027e661",
   "metadata": {},
   "source": [
    "## Revision Example with Circular Test Data\n",
    "\n",
    "For our classification problem, we will use the `make_circles` function. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)\n",
    "\n",
    "The parameters for noise level and relative size of the two circles are such that the task becomes difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77108a4",
   "metadata": {
    "results": "hold"
   },
   "outputs": [],
   "source": [
    "seed(RANDOM_STATE)\n",
    "\n",
    "X, y = make_circles(n_samples=500, factor=0.5, noise=.3, random_state=RANDOM_STATE)\n",
    "\n",
    "feature_1, feature_2 = 0, 1\n",
    "ft_min, ft_max = X.min(), X.max()\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "\n",
    "fig, ax = subplots(figsize=(10, 5), nrows=1, ncols=2)\n",
    "\n",
    "ax[0].scatter(X[:, feature_1], X[:, feature_2], c=y, s=4, cmap='bwr');\n",
    "ax[0].set_xlabel('Feature 1')\n",
    "ax[0].set_ylabel('Feature 1')\n",
    "ax[1].hist(X);\n",
    "ax[1].set_xlabel('Value')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a12c3",
   "metadata": {},
   "source": [
    "For training, we use the same classifiers as in the previous Lesson. We train on the whole data set and then use a meshgrid of the state space for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d032c939",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ft_min, ft_max = -1.5, 1.5\n",
    "\n",
    "# Constructing (2 grids x 300 rows x 300 cols):\n",
    "grid_1, grid_2 = mgrid[ft_min:ft_max:.01, ft_min:ft_max:.01]\n",
    "\n",
    "# We need only the shape for one of the grids (i.e. 300 x  300):\n",
    "grid_shape = grid_1.shape\n",
    "\n",
    "# state space grid for testing\n",
    "new_obs = c_[grid_1.ravel(), grid_2.ravel()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21d22c",
   "metadata": {
    "results": "hold"
   },
   "outputs": [],
   "source": [
    "contour_levels = linspace(0, 1, 6)\n",
    "\n",
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    y_pred = clf.predict(new_obs)\n",
    "    y_pred_grid = y_pred.reshape(grid_shape)\n",
    "    print(\"\")\n",
    "\n",
    "    ax.scatter(X[:, feature_1], X[:, feature_2], c=y, s=1, cmap='bwr_r')\n",
    "    ax.contourf(grid_1, grid_2, y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels);\n",
    "\n",
    "    ax.set_ylim(ft_min, ft_max)\n",
    "    ax.set_xlim(ft_min, ft_max)\n",
    "    ax.set_yticks([ft_min, 0, ft_max])\n",
    "    ax.set_xticks([ft_min, 0, ft_max])\n",
    "    ax.set_title(name, fontsize=10);\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb8842",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Seven of the eight classifiers are able to separate the inner data set from the outer data set successfully. The main difference is that some algorithms ended up with a more rectangular shape of the boundary whereas the others find a more circular form which reflects the original data distribution more closely. One classifier simply fails: SVC (linear). It tries to fit a straight line to separate the classes which in this case is impossible.\n",
    "</p>\n",
    "\n",
    ":::::::::::::::::: callout\n",
    "## Note\n",
    "__Code__: Note how the keyword argument `sharey` is used in the call of `subplots` to have y-axis only labelled once. The name of the classifier is extracted from the dictionary as its key and used to set up the title of each panel.\n",
    "::::::::::::::::::\n",
    "\n",
    "## Metrics\n",
    "\n",
    "We already used the score to evaluate the model performance. Here are some further metrics used in machine learning.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "__Accuracy__ is a metric that evaluates the integrity of the model by comparing true labels with their predicted counterparts. It produces a value between 0 and 1, where 1 is the best possible outcome, and $1 / n_{classes}$ represents the probability of a random guess. See [the Scikit-learn documentation for the accuracy_score.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) The mathematical formula can be found in the [metrics and scoring section of the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score).\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "__Recall__ is a metric that evaluates the ability of a classification model to find true positive labels. The measure produces a scalar value between 0 and 1, where 1 is the perfect outcome.  See [the Scikit-learn documentation for the recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html). The recall is the percentage of true predictions of the overall number of predictions.\n",
    "It is also known as _sensitivity_.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "__Average Precision__, also referred to as AP, is a metric that produces a scalar value for the precision-recall curve between and with being the outcome. The metric obtains this value by weighing:\n",
    "</p>\n",
    "- the mean of precisions (P) at each threshold (n),\n",
    "- the increase in recall (R) from the previous threshold (n-1).\n",
    "\n",
    "The metric is mathematically defined as follows:\n",
    "\n",
    "$$ AP = \\sum_{n}^{}(R_n - R_{n-1}) \\cdot P $$\n",
    "\n",
    "::::::::::::::::::: discussion\n",
    "## Average precision vs AUC\n",
    "<p style='text-align: justify;'>\n",
    "As you may have noticed, the AUC metric also evaluates the area under the precision-recall curve using the trapezoid rule and with linear interpolation. The interpolation, however, may cause the resulting output to be better than it actually is. In other words, the AUC measure evaluates the outcome rather optimistically.\n",
    "</p>\n",
    "\n",
    ":::::::::::::::::::\n",
    "\n",
    "Precision is also called the _positive predictive value_.\n",
    "<p style='text-align: justify;'>\n",
    "__F1 Score__ Another useful metric to evaluate a classification model that relies on precision and recall is the F1 Score, see the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). It is mathematically defined as:\n",
    "</p>\n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{P\\cdot R}{P+R} $$\n",
    "\n",
    "where $P$ and $R$ represent precision and recall, respectively.\n",
    "\n",
    "Wikipedia has a [nice summary of the measures and connections between them](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "In Scikit-learn, these measures can be used in a standardised fashion. Here is an example using the `recall_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fba90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6219d1",
   "metadata": {
    "results": "hold"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "\n",
    "    # Training the model using training data:\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_gr = clf.predict(new_obs)\n",
    "    y_pred_grid = y_pred_gr.reshape(grid_shape)\n",
    "\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    print(\"\")\n",
    "    # Evaluating the score using test data:\n",
    "    score = clf.score(X_test, y_test)\n",
    "    recall = recall_score(y_test, y_predicted)\n",
    "\n",
    "    # Scattering the test data only:\n",
    "    ax.scatter(X_test[:, feature_1], X_test[:, feature_2], c=y_test, s=4, cmap='bwr', marker='.')\n",
    "    print(\"\")\n",
    "    ax.contourf(grid_1, grid_2, y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels)\n",
    "\n",
    "    ax.set_ylim(ft_min, ft_max)\n",
    "    ax.set_xlim(ft_min, ft_max)\n",
    "    ax.set_yticks([-1.5, 0, 1.5])\n",
    "    ax.set_xticks([-1.5, 0, 1.5])\n",
    "\n",
    "    label = '{} - Recall: {:.2f}'.format(name, recall)\n",
    "    ax.set_title(label , fontsize=10);\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09c407",
   "metadata": {},
   "source": [
    "### **Reducing Bias on Test Data**\n",
    "<p style='text-align: justify;'>\n",
    "Whilst `SciKit Learn` provides us with a dedicated function to obtain accuracy, the value it provides depends on how our training and test data have been split. Using the train-test-split, we can randomly shuffle the data to address this very problem. However, this implicitly assumed that our original data followed a specific distribution which is best represented by shuffling the data. That may not always be the case. In practice, we can never fully eliminate this type of bias. What we can do, however, is to split, shuffle, and permute the samples in the original dataset repeatedly to minimise the likelihood of bias.\n",
    "</p>\n",
    "\n",
    "## Permutation Score\n",
    "<p style='text-align: justify;'>\n",
    "When dealing with biological and medical data, the results of machine learning often are not clear-cut. The question remains whether or not to trust a predictor as being truly above chance levels. An effective technique to address this is to randomly shuffle the labels independently of the data. I.e. we permutate only the labels, and check whether the classification score actually decreases. The __permutation score__ then quantifies how trustworthy the result with the correct labels is. See [the Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html) for details.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "Now that we know about evaluation metrics, we are set to properly begin the evaluation process. We can use so-called cross-validators for testing the models if a test is run many times on data with differently permuted labels. To facilitate this, Scikit-learn provides the function `permutation_test_score`.\n",
    "</p>\n",
    "\n",
    "::::::::::::::: callout\n",
    "## Note\n",
    "<p style='text-align: justify;'>\n",
    "The process of cross-validation is computationally expensive, as is the process of repeatedly permuting, fitting, and testing our models. In this context, we will be using both processes to complement each other. This makes the operation time-consuming and slow.\n",
    "</p>\n",
    "\n",
    ":::::::::::::::\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "When possible, Scikit-learn provides us the with ability to use multiple CPU cores to speed up intensive computations through multiprocessing. Where available, this can be achieved by setting the `n_jobs`  argument of a function or a class to the number of CPU cores we wish to use. Conveniently, it can be set to `n_jobs=-1` to use all available CPU cores (see e.g. [the Hyperparameter Tuning section]() below). Here, we have shown the use of only one core with `n_jobs=1` which is computationally slow. You can adjust it according to the machine you are using to make it faster.\n",
    "</p>\n",
    "\n",
    "The keyword argument `n_permutations` is set to 100 by default. You can speed the cross-validation up by choosing a smaller number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "chance = 1 / n_classes\n",
    "\n",
    "fig, axes = subplots(figsize=[16, 12], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(axes.ravel(), classifiers.items()):\n",
    "\n",
    "    score, permutation_scores, pvalue = permutation_test_score(clf, X, y, scoring=\"accuracy\", n_jobs=1,n_permutations=100)\n",
    "\n",
    "    score_label = 'Score: {:.3f}, (p={:.4f})'.format(score, pvalue)\n",
    "    print(\"\")\n",
    "    chance_label = 'Chance: {:.3f}'.format(chance)\n",
    "\n",
    "    ax.hist(permutation_scores)\n",
    "    ax.axvline(score,  c='g', label=score_label,  linewidth=3.0)\n",
    "    ax.axvline(chance, c='r', label=chance_label, linewidth=3.0)\n",
    "    ax.set_title(name, fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51bfbd",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Apart from SVC (linear), all classifiers show satisfactory separation of the permutation test (blue distribution with red mean value) from the data score (green line). Apart from SVC (linear), the p-values are below 0.01.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "Here is a [Scikit-learn example using permutations with the Iris data](https://scikit-learn.org/stable/auto_examples/model_selection/plot_permutation_tests_for_classification.html#sphx-glr-auto-examples-model-selection-plot-permutation-tests-for-classification-py).\n",
    "</p>\n",
    "\n",
    "## Confusion Matrix\n",
    "<p style='text-align: justify;'>\n",
    "Another useful method to evaluate a model and demonstrate its integrity is to produce a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The matrix demonstrates the number of correctly predicted labels against the incorrect ones. As such it can, however, only be used for classification problems with two labels.\n",
    "</p>\n",
    "Scikit-learn provides a [function to create a confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Here is an expanded function to simplify the visualisation of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a97d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_pred, classes, normalize=False, ax=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    y_test (array)\n",
    "    y_pred (array)\n",
    "    classes (array)\n",
    "    normalize (bool) Normalize the results (True), or show them as integer numbers (False).\n",
    "    ax Visualization axis.\n",
    "    The function is an adaptation of a SciKit Learn example.\n",
    "    \"\"\"\n",
    "\n",
    "    from itertools import product\n",
    "    from numpy import asarray, newaxis\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = asarray(cm).astype('float32') /cm.sum(axis=1)[:, newaxis]\n",
    "\n",
    "    if not ax:\n",
    "        from matplotlib.pyplot import subplots, show\n",
    "        fig, ax = subplots()\n",
    "\n",
    "    ticks = range(n_classes)\n",
    "    ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(classes, rotation=90)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = 3*cm.max() / 4\n",
    "    cm_dim = cm.shape\n",
    "\n",
    "    # Matrix indices:\n",
    "    indices_a = range(cm_dim[0])\n",
    "    indices_b = range(cm_dim[1])\n",
    "    # Cartesian product of matrix indices:\n",
    "    indices = product(indices_a, indices_b)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "\n",
    "    for ind_a, ind_b in indices:\n",
    "      label = format(cm[ind_a, ind_b], fmt)\n",
    "      color = \"white\" if cm[ind_a, ind_b] > thresh else \"black\"\n",
    "      ax.text(ind_b, ind_a, label, ha=\"center\", color=color)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9babb62",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class_names = ('False (0)', 'True (1)')\n",
    "\n",
    "fig, axes = subplots(figsize=(17, 12), ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "\n",
    "for ax, (name, clf) in zip(axes.ravel(), classifiers.items()):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True, ax=ax)\n",
    "\n",
    "    ax.set_title(name, fontsize=10);\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572618f7",
   "metadata": {},
   "source": [
    "Ideally, the diagonal fields are both white and the off-diagonal fields maximally dark.\n",
    "\n",
    "## Further Refinements\n",
    "<p style='text-align: justify;'>\n",
    "Once we decide what algorithm to use, we start by training that algorithm with its default settings and evaluate the results. If not satisfied, we can make further adjustments to the __hyper-parameters__ of the algorithm to improve the results.\n",
    "As always in machine learning, it is of great importance that we avoid overfitting, i.e. maintain the generalisability of the model whilst improving its performance.\n",
    "</p>\n",
    "We start by creating a classification problem with 3 features and 2 labels using the `make_classification` function. Data are now displayed in pseudo-3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75486b4d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=3,\n",
    "    n_classes=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=.7,\n",
    "    scale=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "ax.hist(X);\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig, ax = subplots(figsize=(10, 8), subplot_kw=dict(projection='3d'))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, s=5, cmap='bwr');\n",
    "show()\n",
    "\n",
    "fig, axes = subplots(figsize=(12, 3), ncols=3, sharex=True, sharey=True)\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, s=2, cmap='bwr')\n",
    "axes[1].scatter(X[:, 0], X[:, 2], c=y, s=2, cmap='bwr')\n",
    "axes[2].scatter(X[:, 1], X[:, 2], c=y, s=2, cmap='bwr');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20676a6d",
   "metadata": {},
   "source": [
    "::::::::::::::::: callout\n",
    "## Note\n",
    "__Code__: Note the setting up of 3D axis. Some examples with code to learn 3D plotting are [provided in these tutorials](https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html).\n",
    ":::::::::::::::::\n",
    "\n",
    "We can now go ahead and use our classifier dictionary – which contains the classifiers with their default settings – to train and evaluate the models. We use the train-test split to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e739e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.8, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('{:<30} Score: {:.2f}'.format(name, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab017f",
   "metadata": {},
   "source": [
    "### **Normalisation**\n",
    "<p style='text-align: justify;'>\n",
    "Depending on the nature of the data, it might be beneficial to normalise the data before fitting a classifier. This is widely done in machine learning but needs thought in each case.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "Normalisation can be done in various ways. One common way to normalise data is to require that they have mean 0 and variance 1. This is used for example, when calculating the Pearson correlation coefficient. Another popular way in machine learning is to normalise data to Euclidean norm 1. For a data point in an m-dimensional feature space (m is the number of features), the Euclidean norm of a single point (one sample or row) is normalised such that the distance of the point from the origin is 1.\n",
    "</p>\n",
    "Let us first see an example: some data points are spread between 1 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3800b",
   "metadata": {
    "results": "hold"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "some_data = array([[1, 4], [3, 1], [4, 4], [2, 3]])\n",
    "\n",
    "norm_skl         = Normalizer()\n",
    "some_data_normed = norm_skl.fit_transform(some_data)\n",
    "\n",
    "print('Normalised data:', '\\n', some_data_normed)\n",
    "\n",
    "from numpy import amax\n",
    "\n",
    "fig, ax = subplots(nrows=1, ncols=2)\n",
    "\n",
    "scaling = amax(some_data)*1.1\n",
    "\n",
    "ax[0].scatter(some_data[:, 0], some_data[:, 1])\n",
    "ax[0].set_xlim(0, scaling)\n",
    "ax[0].set_ylim(0, scaling)\n",
    "ax[0].set_xlabel('Some data')\n",
    "\n",
    "ax[1].scatter(some_data_normed[:, 0], some_data_normed[:, 1], c='r')\n",
    "ax[1].set_xlim(0, scaling)\n",
    "ax[1].set_ylim(0, scaling);\n",
    "ax[1].set_xlabel('Normalised data')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57cef03",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Effectively, all normalised data are positioned on a circle around the origin with radius 1. Depending on correlations existing between the features this leads to different distortions of the original data.\n",
    "</p>\n",
    "\n",
    "Let us now apply this normalisation to our artificial data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "\n",
    "X_normed = norm.fit_transform(X)\n",
    "\n",
    "fig, ax = subplots(figsize=(8, 8), subplot_kw=dict(projection='3d'))\n",
    "\n",
    "ax.scatter(X_normed[:, 0], X_normed[:, 1], X_normed[:, 2], c=y, s=5, cmap='bwr');\n",
    "ax.view_init(30, 50);\n",
    "show()\n",
    "\n",
    "fig, axes = subplots(figsize=(10, 3), ncols=3, sharex=True, sharey=True)\n",
    "\n",
    "axes[0].scatter(X_normed[:, 0], X_normed[:, 1], c=y, s=2, cmap='bwr')\n",
    "axes[1].scatter(X_normed[:, 0], X_normed[:, 2], c=y, s=2, cmap='bwr')\n",
    "axes[2].scatter(X_normed[:, 1], X_normed[:, 2], c=y, s=2, cmap='bwr');\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ddd66",
   "metadata": {},
   "source": [
    "The normalisation projects the data on the unit sphere. And now we can do the training on the normalised data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed, y, test_size=.8, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print('{:<30} Score: {:.2f}'.format(name, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df000d",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Due to the homogeneous nature of the artificial data, the results here are comparable for the data and their normalised version. But this may change when using data with inconsistent distributions of the columns. For an example, see the [breastcancer data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) used in the assignment.\n",
    "</p>\n",
    "\n",
    "### **Hyperparameter Tuning**\n",
    "<p style='text-align: justify;'>\n",
    "Once we decide on what algorithm to use, we often start by training that algorithm with its default settings and evaluate the results. If not satisfied, we can go further and make adjustments to the hyper-parameters of the algorithm to improve the results.\n",
    "As always in machine learning, it is of great importance that we maintain the generalisability of our model whilst improving its performance.\n",
    "We use the data from the above classification problem with 3 features and 2 labels.\n",
    "</p>\n",
    "\n",
    "### **Progressive Adjustment**\n",
    "<p style='text-align: justify;'>\n",
    "After we have compared original and normalised data and obtained their scores, we now can try to progressively improve the performance of the algorithms.\n",
    "Each classification algorithm uses a unique set of hyper-parameters, the details of which are outlined in their respective documentations on `Scikit-learn`. The optimum parameters are those that produce the best fit whilst maintaining the generalisability of a model. One way to obtain the optimum settings is to test different parameters and compare the model scores over and over again. However, as outlined before, by doing so we may risk _leaking_ our test data, and end up over-fitting the model to the test data. (We also learned above that we can use different cross-validators to address this problem.)\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "`Scikit-learn` provides us with a tool entitled `GridSearchCV` to define different values for different parameters. It then applies different combinations of different parameters to the model and evaluates the outcome using data that it generates from a cross-validation algorithm. Once finished, it provides us with the parameters that produce the best score for our data. This is referred to as progressive adjustment.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "Note that this process can be lengthy, and may need to be refined several times, so it is a good idea to set `n_jobs=-1`  and thereby take advantage of different CPU core on the computer. For demonstration, we use SVC(rbf) as a classifier. With certain problems, its training may lead to poor results with the default parameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600abcc",
   "metadata": {
    "results": "hold"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1, gamma=100, tol=0.0001)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print('{:<30} Score: {:.2f}'.format('SVC (RBF)', score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1ca81",
   "metadata": {},
   "source": [
    "Progressive adjustment of some of the parameters may lead to an improved model.\n",
    "\n",
    "[Check the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) for the meaning and the default values of regularisation parameters `C`, kernel coeffcient `gamma`, and tolerance setting `tol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159f00a",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(C=[1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "                  gamma=[100, 1000, 10000, 100000],\n",
    "                  tol=[1e-4, 1e-3, 1e-2, 1e-1])\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=RANDOM_STATE)\n",
    "clf = SVC(kernel='rbf', random_state=RANDOM_STATE)\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=cv, n_jobs=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"ORIGINAL: Best parameters {}   Score: {:.2f}\".format(grid.best_params_, grid.best_score_))\n",
    "\n",
    "grid.fit(X_normed, y)\n",
    "\n",
    "print(\"NORMED:   Best parameters {}    Score {:.2f}\".format(grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871497b",
   "metadata": {},
   "source": [
    "```\n",
    "ORIGINAL: Best parameters {'C': 0.0001, 'gamma': 1000, 'tol': 0.0001}   Score: 0.65\n",
    "\n",
    "NORMED:   Best parameters {'C': 1, 'gamma': 100, 'tol': 0.0001}    Score 0.75\n",
    "```\n",
    "<p style='text-align: justify;'>\n",
    "In this case, while both optimised scores are better than the original one, there is also a notable improvement when using the normalised data. Let us similarly check the Random Forest classifier, first with default settings.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a59e2b",
   "metadata": {
    "lines_to_next_cell": 2,
    "results": "hold"
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print('{:<30} Score: {:.2f}'.format('Random Forest', score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeee120",
   "metadata": {},
   "source": [
    "And now a grid over [some of its parameters](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcaf52",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(\n",
    "    n_estimators=[5, 10, 15, 20, 50, 60, 70],\n",
    "    max_features=[None, 'auto', 'sqrt', 'log2'],\n",
    "    min_samples_split=[2, 3, 4, 5],\n",
    "    max_depth=[1, 2, 3, 4]\n",
    ")\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=cv, n_jobs=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"ORIGINAL: Best parameters {}   Score: {:.2f}\".format(grid.best_params_, grid.best_score_))\n",
    "\n",
    "grid.fit(X_normed, y)\n",
    "\n",
    "print(\"NORMED:   Best parameters {}    Score {:.2f}\".format(grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d0ac0",
   "metadata": {},
   "source": [
    "```\n",
    "ORIGINAL: Best parameters {'max_depth': 4, 'max_features': None, 'min_samples_split': 2, 'n_estimators': 15}   Score: 0.84\n",
    "\n",
    "NORMED:   Best parameters {'max_depth': 3, 'max_features': 'auto', 'min_samples_split': 4, 'n_estimators': 10}    Score 0.81\n",
    "\n",
    "```\n",
    "\n",
    "In this case, our (arbitrary) search did not lead to a substantial improvement. This shows that the default settings are in fact a good starting point.\n",
    "\n",
    "### **Leakage in progressive adjustments**\n",
    "<p style='text-align: justify;'>\n",
    "We have already highlighted unequivocally the importance of not exposing our test data to our model during the training process; but where does training end? After deciding on an algorithm, we often attempt to improve its performance by adjusting its hyper-parameters as done above. We make these adjustments on our model repeatedly until we obtain optimal results in a specific metric that scores the performances based exclusively on our test data. In such cases, we risk _leaking_ our test data and thereby over-fit our model to the test data through progressive adjustments. This means that the evaluation metrics on the generalisability of our model are no longer reliable.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "One way to address this problem is to split our original data into 3 different datasets: training, test, and validation. Whilst this is a valid approach that may be used in specific circumstances, it might also introduce new problems, e.g. after splitting the available data into 3 subsets, there might just not be enough data to train the classifier properly.\n",
    "</p>\n",
    "\n",
    "See for example the discussion in part 2 of this [paper on predictive modelling for brain stimulation](https://www.brainstimjrnl.com/article/S1935-861X(21)00236-9/fulltext). The above leaking is there referred to as \"snooping\".\n",
    "\n",
    "<br />\n",
    "\n",
    "### **Lesson Assignment** [(Tutorial Video)](https://www.youtube.com/watch?app=desktop&v=5UzoWit0Ewc) {#assign}\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "The assignment for this lesson consists of the questions shown below in the next section and can be accessed via GitHub classroom.\n",
    "\n",
    "- For **L2D online cohort**, please click this link: [**GitHub classroom assignment link**](https://classroom.github.com/a/SPvIcYNb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfc6bf",
   "metadata": {},
   "source": [
    ":::::::::::::::: callout\n",
    "## Note\n",
    "\n",
    "1. You will need to login to your GitHub account.\n",
    "\n",
    "2. Choose your name from the list of students.\n",
    "\n",
    "3. Accept the assignment.\n",
    "\n",
    "4. Refresh the page.\n",
    "\n",
    "By doing these steps, you will be able to access the assignment repository at GitHub. Instructions for completing the assignment are displayed beneath the file browser containing all the files you will need. These instructions are contained in the `README.md` file.\n",
    "\n",
    "::::::::::::::::\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::::: challenge\n",
    "\n",
    "#### Assignmet Questions\n",
    "\n",
    "As a suggestion, take the [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n",
    "\n",
    "1. Using all features create a summary boxplot to see the medians and distributions of the features.\n",
    "\n",
    "2. Train the above introduced classifiers using the train_test split to generate testing and training data and pick a small training set of e.g. 10% to make the classification task difficult. Obtain the recall scores to compare classifiers.\n",
    "\n",
    "3. Plot the confusion matrix for each case.\n",
    "\n",
    "4. Do a permutation test with default settings to get the p-values to reject the null hypothesis that the scores are compatible with random predictions. If it takes too long, reduce `n_permutations`.\n",
    "\n",
    "5. Repeat the workflow with normalised data and compare the results.\n",
    "\n",
    "6. Perform a hyperparameter tuning with the Random Forest classifier. For the optimal parameter settings, re-run the training and plot the feature importances to see the contributions of each feature to the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76746a6b",
   "metadata": {},
   "source": [
    "The breast cancer data can be imported from the `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f36434",
   "metadata": {},
   "source": [
    "Feel free to try and do any other testing or plotting that you find important. This assignment is not meant to get a correct answer. It should help you to increase flexibility when facing a complex machine learning problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a2ddc",
   "metadata": {},
   "source": [
    "::::::::::::::::::::: solution\n",
    "\n",
    "## Please check these solutions only after submitting the assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5417862",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "from numpy import mgrid, linspace, c_, arange, mean, array\n",
    "from numpy.random import uniform, seed\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.pyplot import subplots, axes, scatter, xticks\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RANDOM_STATE = 111\n",
    "\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Random Forest)': AdaBoostClassifier(RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Extra Tree)': AdaBoostClassifier(ExtraTreesClassifier(random_state=RANDOM_STATE)),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'SVC (RBF)': SVC(random_state=RANDOM_STATE),\n",
    "    'SVC (Linear)': LinearSVC(random_state=RANDOM_STATE, dual=False),\n",
    "    'Multi-layer Perceptron': MLPClassifier(max_iter=5000, random_state=RANDOM_STATE)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36468d33",
   "metadata": {},
   "source": [
    "Notice that the linear Support Vector classifier is imported with the keyword argument `dual=False`. This is to reduce the number of (pink) warnings that occur when the classifier struggles to find a good solution.\n",
    "\n",
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7ebec",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6354393",
   "metadata": {},
   "source": [
    "To get the feature names, you can access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c759fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123599da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame(X)\n",
    "\n",
    "df.boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a633bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Data are differently distributed. Features with indices 3 and 23 have largest medians and variances.\n",
    "\n",
    "## Q2 Train-test split and classification of original data\n",
    "\n",
    "Only a small training set is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacf8c6",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.9, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e22623c",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    # Training the model using training data:\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    # Evaluating the score using test data:\n",
    "    score = clf.score(X_test, y_test)\n",
    "    recall = recall_score(y_test, y_predicted)\n",
    "\n",
    "    # Scattering two features of test data only:\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=4, cmap='bwr', marker='.')\n",
    "\n",
    "    label = '{} - Recall Score: {:.2f}'.format(name, recall)\n",
    "    ax.set_title(label , fontsize=10);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a2757",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Q3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8eb1fe",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_pred, classes, normalize=False, ax=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    y_test (array)\n",
    "    y_pred (array)\n",
    "    classes (array)\n",
    "    normalize (bool) Normalize the results (True), or show them as integer numbers (False).\n",
    "    ax Visualization axis.\n",
    "    The function is an adaptation of a SciKit Learn example.\n",
    "    \"\"\"\n",
    "\n",
    "    from itertools import product\n",
    "    from numpy import asarray, newaxis\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = asarray(cm).astype('float32') / cm.sum(axis=1)[:, newaxis]\n",
    "\n",
    "    if not ax:\n",
    "        from matplotlib.pyplot import subplots\n",
    "        fig, ax = subplots()\n",
    "\n",
    "    ticks = range(n_classes)\n",
    "    ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(classes, rotation=90)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = 3*cm.max() / 4\n",
    "    cm_dim = cm.shape\n",
    "\n",
    "    # Matrix indices:\n",
    "    indices_a = range(cm_dim[0])\n",
    "    indices_b = range(cm_dim[1])\n",
    "    # Cartesian product of matrix indices:\n",
    "    indices = product(indices_a, indices_b)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "\n",
    "    for ind_a, ind_b in indices:\n",
    "        label = format(cm[ind_a, ind_b], fmt)\n",
    "        color = \"white\" if cm[ind_a, ind_b] > thresh else \"black\"\n",
    "        ax.text(ind_b, ind_a, label, ha=\"center\", color=color)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcee2ae",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "class_names = ('False (0)', 'True (1)')\n",
    "\n",
    "fig, axes = subplots(figsize=(17, 12), ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(axes.ravel(), classifiers.items()):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True, ax=ax)\n",
    "\n",
    "    ax.set_title(name, fontsize=10);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c88956",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Q4 Permutation Test Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c63b3d",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "chance = 1 / n_classes\n",
    "\n",
    "fig, axes = subplots(figsize=[16, 12], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(axes.ravel(), classifiers.items()):\n",
    "\n",
    "    score, permutation_scores, pvalue = permutation_test_score(clf, X, y,\n",
    "                                                               scoring=\"accuracy\",\n",
    "                                                               n_jobs=1,\n",
    "                                                               n_permutations=100)\n",
    "\n",
    "    score_label = 'Score: {:.3f}, (p={:.4f})'.format(score, pvalue)\n",
    "\n",
    "    chance_label = 'Chance: {:.3f}'.format(chance)\n",
    "\n",
    "    ax.hist(permutation_scores)\n",
    "    ax.set_ylim(0, 30)\n",
    "    ax.axvline(score,  c='g', label=score_label,  linewidth=3.0)\n",
    "    ax.axvline(chance, c='r', label=chance_label, linewidth=3.0)\n",
    "    ax.set_title(name, fontsize=10)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c1851",
   "metadata": {},
   "source": [
    "The classification result is good in that the green score for the data is separate from the score distribution of permutated data. However, the permutated data are distributed systematically above 0.5. This is presumably due to the strongly skewed distributions of some of the features (see the boxplots above). For both SVCs, there are cases where the classifier fails to converge, and thus data are missing. (There would have been many warnings, but warnings were switched off (see abvove under 'Import Functions').\n",
    "\n",
    "## Q5 Normalisation\n",
    "\n",
    "The code for three common scalers is shown below. Figures were obtained with the `Normaliser`. Note that this changes the y-scale of the data, but does not affect the skewness of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d6ec4",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm_skl       = Normalizer()\n",
    "X_normed = norm_skl.fit_transform(X)\n",
    "\n",
    "X_normed.shape\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame(X_normed)\n",
    "\n",
    "df.boxplot();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6da11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Train-test split and classification of normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b6592",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "X_normed_train, X_normed_test, y_train, y_test = train_test_split(X_normed, y, test_size=.9, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "print(X_normed_train.shape, X_normed_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f7635",
   "metadata": {
    "eval": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    # Training the model using trainiang data:\n",
    "    clf.fit(X_normed_train, y_train)\n",
    "\n",
    "    y_predicted = clf.predict(X_normed_test)\n",
    "\n",
    "    # Evaluating the score using test data:\n",
    "    score = clf.score(X_normed_test, y_test)\n",
    "    recall = recall_score(y_test, y_predicted)\n",
    "\n",
    "    # Scattering two features of test data only:\n",
    "    ax.scatter(X_normed_test[:, 0], X_normed_test[:, 1], c=y_test, s=4, cmap='bwr', marker='.')\n",
    "\n",
    "    label = '{} - Recall Score: {:.2f}'.format(name, recall)\n",
    "    ax.set_title(label , fontsize=10);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e30856",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the normalised data, the recall score is high. The SVCs even achieve scores of 1.0. The Recall is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20561d8b",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "class_names = ('False (0)', 'True (1)')\n",
    "\n",
    "fig, axes = subplots(figsize=(17, 12), ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(axes.ravel(), classifiers.items()):\n",
    "\n",
    "    clf.fit(X_normed_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_normed_test)\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True, ax=ax)\n",
    "\n",
    "    ax.set_title(name, fontsize=10);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfa0ed",
   "metadata": {},
   "source": [
    "Notice how both SVC perform badly! All true positive were found (see above) but they struggled to detect the false negatives. In this specific case, the single recall score would be quite misleading.\n",
    "\n",
    "If instead of the Normaliser, we apply the Standard Scaler, yielding mean 0 and variance 1 for all features, the results look a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838268d",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_skl       = StandardScaler()\n",
    "X_normed = std_skl.fit_transform(X)\n",
    "\n",
    "df = DataFrame(X_normed)\n",
    "\n",
    "df.boxplot();\n",
    "\n",
    "\n",
    "X_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e817f",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "X_normed_train, X_normed_test, y_train, y_test = train_test_split(X_normed, y, test_size=.9, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "print(X_normed_train.shape, X_normed_test.shape)\n",
    "\n",
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    # Training the model using trainiang data:\n",
    "    clf.fit(X_normed_train, y_train)\n",
    "\n",
    "    y_predicted = clf.predict(X_normed_test)\n",
    "\n",
    "    # Evaluating the score using test data:\n",
    "    score = clf.score(X_normed_test, y_test)\n",
    "    recall = recall_score(y_test, y_predicted)\n",
    "\n",
    "    # Scattering two features of test data only:\n",
    "    ax.scatter(X_normed_test[:, 0], X_normed_test[:, 1], c=y_test, s=4, cmap='bwr', marker='.')\n",
    "\n",
    "    label = '{} - Recall Score: {:.2f}'.format(name, recall)\n",
    "    ax.set_title(label , fontsize=10);\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eea046",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "class_names = ('False (0)', 'True (1)')\n",
    "\n",
    "fig, axes = subplots(figsize=(17, 12), ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(axes.ravel(), classifiers.items()):\n",
    "\n",
    "    clf.fit(X_normed_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_normed_test)\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True, ax=ax)\n",
    "\n",
    "    ax.set_title(name, fontsize=10);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e5e70",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Q6 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab641f9",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print('Score: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb501edd",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    n_estimators=[30, 50, 70, 90],\n",
    "    max_features=[None, 'auto', 'sqrt', 'log2'],\n",
    "    min_samples_split=[2, 3, 4],\n",
    "    max_depth=[2, 3, 4, 5, 6]\n",
    ")\n",
    "\n",
    "cv = StratifiedShuffleSplit(test_size=0.9, random_state=RANDOM_STATE)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=cv, n_jobs=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"ORIGINAL data: Best parameters {}   Score: {:.2f}\".format(grid.best_params_, grid.best_score_))\n",
    "\n",
    "grid.fit(X_normed, y)\n",
    "\n",
    "print(\"NORMED data:   Best parameters {}    Score {:.2f}\".format(grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe33009",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=4,\n",
    "                             max_features=None,\n",
    "                             min_samples_split=2,\n",
    "                             n_estimators=50,\n",
    "                             random_state=RANDOM_STATE)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print('Random Forest Score: {:.2f}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d952e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Arbitrary parameter searches do not necessarily lead to improved performance. The reason our score differs from the score reported in the grid search is that the grid search used 10 splits into different train and test data.\n",
    "\n",
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aabab8",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "\n",
    "bins = arange(importances.shape[0])\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "ax.bar(bins, importances);\n",
    "ax.set_ylabel('Feature Importance', fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed71e5",
   "metadata": {
    "eval": false
   },
   "outputs": [],
   "source": [
    "# Most important features\n",
    "threshold = 0.1\n",
    "\n",
    "feature_indices = bins[importances > threshold]\n",
    "\n",
    "feature_names = data.feature_names[feature_indices]\n",
    "\n",
    "print('Indices of features with importance above ', threshold, ':', sep='')\n",
    "print(list(feature_indices))\n",
    "print('Feature Name(s):', feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c7fe1",
   "metadata": {},
   "source": [
    "It turns out that with the used settings, the classification is dominated by a single feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece4aef",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::\n",
    "::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "<br />\n",
    "\n",
    "### **Forum for Questions** [(Tutorial Video)](https://www.youtube.com/watch?app=desktop&v=N5N7QbLwztQ){#forum}\n",
    "\n",
    "----------------------\n",
    "\n",
    "Any questions related to this lesson, code or assignment can be posted at our [forum](https://github.com/orgs/L2D-June2023/discussions) on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fabed",
   "metadata": {},
   "source": [
    ":::::::::::::::: callout\n",
    "## Note\n",
    "\n",
    "The forum can only be accessed via your GitHub account once you have accepted the invitation to be included in the team.\n",
    "\n",
    "::::::::::::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdf3bd",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- The function `permutation_test_score` evaluates the significance of a cross-validated score with permutations.\n",
    "- Confusion matrix demonstrates the number of correctly predicted labels against the incorrect ones.\n",
    "- Adjustment of hyper-parameters of the algorithms may improve the results.\n",
    "- `GridSearchCV` is a tool to simultaneously define different values of different parameters for optimisation.\n",
    "- Progressive adjustments may lead to model over-fitting and require a validation data set.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "[r-markdown]: https://rmarkdown.rstudio.com/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "results,eval,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
