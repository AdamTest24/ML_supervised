{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a031d994",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Improvement\"\n",
    "teaching: 10\n",
    "exercises: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25939cec",
   "metadata": {},
   "source": [
    "[**Download Chapter pdf**](02-improvement.md.pdf)\n",
    "\n",
    "[**Download Chapter notebook (ipynb)**](02-improvement.ipynb)\n",
    "\n",
    "[<span style=\"color: rgb(255, 0, 0);\">**Mandatory Lesson Feedback Survey**</span>](https://docs.google.com/forms/d/e/1FAIpQLSdr0capF7jloJhPH3Pki1B3LZoKOG16poOpuVJ7SL2LkwLHQA/viewform?pli=1)\n",
    "\n",
    "## Submissions\n",
    "- [**Lesson Assignment**](#assign)\n",
    "\n",
    "- [**Lesson Forum**](#forum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe65f91",
   "metadata": {},
   "source": [
    "- How to deal with complex classification problems?\n",
    "- Why is it important to use different classification algorithms?\n",
    "- What is the best way to find the optimal classifier?\n",
    "- How can we avoid over-fitting of data?\n",
    "- How do we evaluate the performance of classifiers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393bf33",
   "metadata": {},
   "source": [
    "- Understanding complex training and testing data.\n",
    "- Comparison of different model classes.\n",
    "- Explaining the stratified shuffle split.\n",
    "- Evaluation of classification - the ROC and AUC curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88d246",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LH3cUN7WXlg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "<br>\n",
    "<p align = \"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/GvUvwHmTXUs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "<br>\n",
    "<p align = \"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xjpQRhtY1l0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "<br>\n",
    "<p align = \"center\">\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nEyt1Ht8GOk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d5cd0",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "1. From now on the code will become more complex. When copied, the code should run without errors with the given data sets. (Please report any errors thrown when running the code without modifications). \n",
    "\n",
    "2. Make a copy of the notebook and start experimenting by modifying part of the code and comparing the outcome. Modifying existing code is one of the successful strategies when learning to programme as a non-programmer. \n",
    "\n",
    "3. The first resource to consult when facing bugs are the official documentations, be it Python, Numpy, SciKit Learn or other.\n",
    "\n",
    "4. If you formulate a problem adequately, often there may be good answers on [Stack Overflow](https://stackoverflow.com).\n",
    "\n",
    "5. Sometimes, simply copying and pasting an error message into the search engine can point you to the solution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbd1ef",
   "metadata": {},
   "source": [
    "### **Import functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import mgrid, linspace, c_, arange, mean, array\n",
    "from numpy.random import uniform, seed\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.pyplot import subplots, axes, scatter, xticks, show\n",
    "\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cb9bd",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "<p style='text-align: justify;'>\n",
    "We would like to test several machine learning models' ability to deal with a complicated task. A complicated task is one where the topology of the labelled data is not trivially separable into classes by (hyper)planes, e.g. by a straight line in a scatter plot.\n",
    "</p>\n",
    "\n",
    "Our example is one class of data organised in a doughnut shape and the other class contained within the first doughnut forming a doughnut-within-a-doughnut.\n",
    "\n",
    "Here is the function code to create these data, followed by a function call to produce a figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3b2ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def make_torus_3D(n_samples=100, shuffle=True, noise=None, random_state=None,\n",
    "                 factor=.8):\n",
    "    \"\"\"Make a large torus containing a smaller torus in 3d.\n",
    "\n",
    "    A toy dataset to visualize clustering and classification\n",
    "    algorithms.\n",
    "\n",
    "    Read more in the :ref:`User Guide <sample_generators>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        The total number of points generated. If odd, the inner circle will\n",
    "        have one point more than the outer circle.\n",
    "\n",
    "    shuffle : bool, optional (default=True)\n",
    "        Whether to shuffle the samples.\n",
    "\n",
    "    noise : double or None (default=None)\n",
    "        Standard deviation of Gaussian noise added to the data.\n",
    "\n",
    "    random_state : int, RandomState instance or None (default)\n",
    "        Determines random number generation for dataset shuffling and noise.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    factor : 0 < double < 1 (default=.8)\n",
    "        Scale factor between inner and outer circle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array of shape [n_samples, 2]\n",
    "        The generated samples.\n",
    "\n",
    "    y : array of shape [n_samples]\n",
    "        The integer labels (0 or 1) for class membership of each sample.\n",
    "    \"\"\"\n",
    "    from numpy import pi, linspace, cos, sin, append, ones, zeros, hstack, vstack, intp\n",
    "    from sklearn.utils import check_random_state, shuffle\n",
    "\n",
    "    if factor >= 1 or factor < 0:\n",
    "        raise ValueError(\"'factor' has to be between 0 and 1.\")\n",
    "\n",
    "    n_samples_out = n_samples // 2\n",
    "    n_samples_in = n_samples - n_samples_out\n",
    "\n",
    "    co, ao, ci, ai = 3, 1, 3.6, 0.2\n",
    "    generator = check_random_state(random_state)\n",
    "    # to not have the first point = last point, we set endpoint=False\n",
    "    linspace_out = linspace(0, 2 * pi, n_samples_out, endpoint=False)\n",
    "    linspace_in  = linspace(0, 2 * pi, n_samples_in,  endpoint=False)\n",
    "    outer_circ_x = (co+ao*cos(linspace_out)) * cos(linspace_out*61.1)\n",
    "    outer_circ_y = (co+ao*cos(linspace_out)) * sin(linspace_out*61.1)\n",
    "    outer_circ_z =    ao*sin(linspace_out)\n",
    "    \n",
    "    inner_circ_x = (ci+ai*cos(linspace_in)) * cos(linspace_in*61.1)* factor\n",
    "    inner_circ_y = (ci+ai*cos(linspace_in)) * sin(linspace_in*61.1) * factor\n",
    "    inner_circ_z =    ai*sin(linspace_in) * factor\n",
    "\n",
    "    X = vstack([append(outer_circ_x, inner_circ_x),\n",
    "                append(outer_circ_y, inner_circ_y),\n",
    "                append(outer_circ_z, inner_circ_z)]).T\n",
    "            \n",
    "    y = hstack([zeros(n_samples_out, dtype=intp),\n",
    "                   ones(n_samples_in, dtype=intp)])\n",
    "    \n",
    "    \n",
    "    if shuffle:\n",
    "        X, y = shuffle(X, y, random_state=generator)\n",
    "\n",
    "    if noise is not None:\n",
    "        X += generator.normal(scale=noise, size=X.shape)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606cd6e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785576ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE  = 12345\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "X, y = make_torus_3D(n_samples=2000, factor=.9, noise=.001, random_state=RANDOM_STATE)\n",
    "\n",
    "feature_1, feature_2, feature_3 = 0, 1, 2\n",
    "ft_min, ft_max = X.min(), X.max()\n",
    "\n",
    "fig, ax = subplots(figsize=(12, 9))\n",
    "\n",
    "ax = axes(projection=\"3d\")\n",
    "\n",
    "im = ax.scatter3D(X[:, feature_1], X[:, feature_2], X[:, feature_3], marker='o', s=20, c=y, cmap='bwr');\n",
    "\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "\n",
    "# Angles to pick the perspective\n",
    "ax.view_init(30, 50);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061a1ba",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-3-1.png\" width=\"1152\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The challenge here is that the only way to separate the data of the two labels from each other is to find a separating border that lies between the blue and the red doughnut (mathematically: torus) and itself is a torus, i.e. a complex topology. Similarly, one can test to separate one class of data that lie on the surface of a sphere and then have data on another sphere embedded within it. Typically, it is unknown what type of high-dimensional topologies is present in biological data. As such it is not clear at the outset which classification strategy will work best. Let us start with a simpler example.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7a428",
   "metadata": {},
   "source": [
    "## Traing a variety of machine learning models\n",
    "\n",
    "`SciKit Learn` provides the means to generate practice datasets with specific qualities. In this section, we will use the `make_circles` function. (see the [documentations](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)):\n",
    "\n",
    "### **Circular Test Data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE  = 1234\n",
    "seed(RANDOM_STATE)\n",
    "\n",
    "X, y = make_circles(n_samples=500, factor=0.3, noise=.05, random_state=RANDOM_STATE)\n",
    "\n",
    "feature_1, feature_2 = 0, 1\n",
    "ft_min, ft_max = X.min(), X.max()\n",
    "\n",
    "print('Shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1ec24",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "Shape of X: (500, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(10, 5), nrows=1, ncols=2)\n",
    "ax[0].scatter(X[:, feature_1], X[:, feature_2], c=y, s=4, cmap='bwr');\n",
    "ax[1].hist(X);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b6532",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-4-3.png\" width=\"960\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The function yields only two features. The reason is that with two features we can visualise the complete state space in a two-dimensional scatter plot. The data of both labels are organised along a ring. There is a certain amount of randomness added to create data distributed normally around the ring. \n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "The tricky thing about such a data distribution is that in a standard view of the data, the histogram, the clear state space organisation is not visible. There are e.g. no two distinct mean values of the distributions. Also, while the two features are clearly dependent on each other (as seen in the scatter plot), it is not possible to regress one with the other by means of fits of the type y = f(x).\n",
    "</p>\n",
    "\n",
    "We will now use different classes of machine learning models to fit to these labelled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b7b4b",
   "metadata": {},
   "source": [
    "### **Classification Algorithms**\n",
    "\n",
    "Different classification algorithms approach problems differently. Let us name the algorithms in `SciKit Learn`.\n",
    "\n",
    "`SciKit Learn` provides the following algorithms for classification problems:\n",
    "\n",
    "- Ensemble: Averaging:\n",
    "    - Random Forest \n",
    "    - Extra Tree \n",
    "    - Isolation Forest \n",
    "    - Bagging\n",
    "    - Voting \n",
    "   \n",
    "- Boosting:\n",
    "    - Gradient Boosting \n",
    "    - AdaBoost\n",
    "    \n",
    "- Decision Trees: \n",
    "    - Decision Tree \n",
    "    - Extra Tree\n",
    "  \n",
    "- Nearest Neighbour: \n",
    "    - K Nearest Neighbour \n",
    "    - Radius Neighbours \n",
    "    - Nearest Centroid\n",
    "    \n",
    "- Support Vector Machine: \n",
    "    - with non-linear kernel:\n",
    "        - Radial Basis Function (RBF) Polynomial\n",
    "        - Sigmoid\n",
    "    - with linear kernel: \n",
    "        - Linear kernel\n",
    "    - parametrised with non-linear kernel: \n",
    "        - Nu-Support Vector Classification\n",
    "        \n",
    "- Neural Networks: \n",
    "    - Multi-layer Perceptron\n",
    "    - Gaussian: \n",
    "        - Gaussian Process\n",
    "    - Linear Models: \n",
    "        - Logistic Regression\n",
    "        - Passive Aggressive\n",
    "        - Ridge\n",
    "        - Linear classifiers with Stochastic Gradient Descent\n",
    "\n",
    "- Baysian: \n",
    "    - Bernoulli \n",
    "    - Multinomial \n",
    "    - Complement\n",
    "        \n",
    "Some of these algorithms require a more in-depth understanding of how they work. To that end, we only review the performance of those that are easier to implement and adjust. \n",
    "\n",
    "__AdaBoost__\n",
    "<p style='text-align: justify;'>\n",
    "The AdaBoost algorithm is special in that it does not work on its own; instead, it complements another ensemble algorithm (e.g. Random Forest) and _boosts_ its performance by weighing the training data through a boosting algorithm. Note that boosting the performance does not necessarily translate into a better fit. This is because boosting algorithms are generally robust against over-fitting, meaning that they always try to produce generalisable models.\n",
    "</p>\n",
    "\n",
    "__Seeding__\n",
    "<p style='text-align: justify;'>\n",
    "Most machine learning algorithms rely on random number generation to produce results. Therefore, one simple, but important adjustment is to `seed` the number generator, and thereby making our comparisons more consistent; i.e. ensure that all models use the same set of random numbers. Almost all SciKit Learn models take an argument called `random_state`, which takes an integer number to seed the random number generator.\n",
    "</p>\n",
    "\n",
    "### **Training and Testing**\n",
    "\n",
    "Here is code to import a number of classifiers from SciKit Learn, fit them to the training data and predict the (complete) state space. The result is plotted below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2704200",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Random Forest)': AdaBoostClassifier(RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Extra Tree)': AdaBoostClassifier(ExtraTreesClassifier(random_state=RANDOM_STATE)),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'SVC (RBF)': SVC(random_state=RANDOM_STATE),\n",
    "    'SVC (Linear)': LinearSVC(random_state=RANDOM_STATE),\n",
    "    'Multi-layer Perceptron': MLPClassifier(max_iter=5000, random_state=RANDOM_STATE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb96d2e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ae248",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ft_min, ft_max = -1.5, 1.5\n",
    "\n",
    "# Constructing (2 grids x 300 rows x 300 cols):\n",
    "grid_1, grid_2 = mgrid[ft_min:ft_max:.01, ft_min:ft_max:.01] \n",
    "\n",
    "# We need only the shape for one of the grids (i.e. 300 x  300):\n",
    "grid_shape = grid_1.shape\n",
    "\n",
    "# state space grid for testing\n",
    "new_obs = c_[grid_1.ravel(), grid_2.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db849470",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_levels = linspace(0, 1, 6)\n",
    "\n",
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(new_obs)\n",
    "    y_pred_grid = y_pred.reshape(grid_shape)\n",
    "\n",
    "    ax.scatter(X[:, feature_1], X[:, feature_2], c=y, s=1, cmap='bwr_r')\n",
    "    ax.contourf(grid_1, grid_2, y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels)\n",
    "    ax.set_ylim(ft_min, ft_max)\n",
    "    ax.set_xlim(ft_min, ft_max)\n",
    "    ax.set_yticks([-1.5, 0, 1.5])\n",
    "    ax.set_xticks([-1.5, 0, 1.5])\n",
    "    ax.set_title(name, fontsize=10);\n",
    "    \n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e18db8",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-7-5.png\" width=\"1440\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Seven of the eight classifiers were able to separate the inner data set from the outer data set successfully. The main difference is that some algorithms ended up with a more rectangular shape of the boundary whereas the others found a more circular form which reflects the original data distribution more closely. One classifier simply fails: the support vector classifier (SVC) with linear basis functions: it tries to fit a straight line to separate the classes which in this case is impossible. \n",
    "</p>\n",
    "\n",
    "### **The Train-Test Split**\n",
    "<p style='text-align: justify;'>\n",
    "We will now modify our workflow to avoid the need to create separate testing data (the typical situation when dealing with recorded data). For this we start with a data set of n labelled samples. Of these n samples, a certain percentage is used for training (using the provided labels) and the rest for testing (withholding the labels). The testing data then do not need to be prepared separately. \n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "The function we use is `train_test_split` from SciKit Learn. A nice feature of this function is that it tries to preserve the ratio of labels in the split. E.g. if the data contain 70% of `True` and 30 % of `False` labels, the algorithm tries to preserve this ratio in the split as good as possible: around 70% of the training data and of the testing data will have the `True` label. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a31886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(n_samples=1000, factor=0.3, noise=.05, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bca5bf",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(700, 2) (300, 2)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Here is an illustration of the two sets of data. The splitting into testing and training data is done randomly. Picking test data randomly is particularly important for real data as it helps to reduce potential bias in the recording order.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(7, 6), ncols=2, nrows=2, sharex=True)\n",
    "\n",
    "ax[0, 0].scatter(X_train[:, feature_1], X_train[:, feature_2], c=y_train, s=4, cmap='bwr')\n",
    "ax[0, 1].scatter(X_test[:, feature_1], X_test[:, feature_2], c=y_test, s=4, cmap='bwr')\n",
    "\n",
    "ax[1, 0].hist(X_train)\n",
    "ax[1, 1].hist(X_test)\n",
    "ax[0, 0].set_title('Training data')\n",
    "ax[0, 1].set_title('Test data')\n",
    "\n",
    "ax[0, 0].set_ylim(ft_min, ft_max)\n",
    "ax[0, 1].set_ylim(ft_min, ft_max)\n",
    "ax[1, 0].set_ylim(0, 100)\n",
    "ax[1, 1].set_ylim(0, 100);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b43c6",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-9-7.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Now we can repeat the training with this split dataset using eight types of models as above.\n",
    "<p style='text-align: justify;'>\n",
    "To compare the model performances, we use __scoring__: the method `.score` takes as input arguments the testing samples and their true labels. It then uses the model predictions to calculate the fraction of labels in the testing data that were predicted correctly.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "There are different techniques to evaluate the performance, but the `.score`  method provides a quick, simple, and handy way to assess a model. As far as classification algorithms in SciKit Learn are concerned, the method usually produces the __mean accuracy__, which is between 0 and 1; and the higher the score, the better the fit.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a91115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    # Training the model using training data:     \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(new_obs)\n",
    "    y_pred_grid = y_pred.reshape(grid_shape)\n",
    "\n",
    "    # Evaluating the score using test data:\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Scattering the test data only:     \n",
    "    ax.scatter(X_test[:, feature_1], X_test[:, feature_2], c=y_test, s=4, cmap='bwr', marker='.')\n",
    "\n",
    "    ax.contourf(grid_1, grid_2, y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels)\n",
    "#    ax.contourf(grid[0], grid[1], y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels)\n",
    "\n",
    "    ax.set_ylim(ft_min, ft_max)\n",
    "    ax.set_xlim(ft_min, ft_max)\n",
    "    ax.set_yticks([-1.5, 0, 1.5])\n",
    "    ax.set_xticks([-1.5, 0, 1.5])\n",
    "\n",
    "    label = '{} - Score: {:.2f}'.format(name, score)\n",
    "    ax.set_title(label , fontsize=10);\n",
    "    \n",
    "show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca239be",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-10-9.png\" width=\"1440\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Here, we only plotted the test data, those that were classified based on the trained model. The gray area shows the result of the classification: within the gray area the prediction is 1 (the red samples) and outside it is 0 (the blue samples). The result is that testing data are classified correctly in all but one of the classifiers, so their performance is 1, or 100 %. This is excellent because it demonstrates that most classifiers are able to deal with embedded topologies.\n",
    "</p>\n",
    "\n",
    "Let us now repeat the procedure with a higher level of noise to make the task more complicated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, factor=.5, noise=.3, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "fig, ax = subplots(figsize=(7, 6), ncols=2, nrows=2, sharex=True)\n",
    "\n",
    "ax[0, 0].scatter(X_train[:, feature_1], X_train[:, feature_2], c=y_train, s=4, cmap='bwr')\n",
    "ax[0, 1].scatter(X_test[:, feature_1], X_test[:, feature_2], c=y_test, s=4, cmap='bwr')\n",
    "\n",
    "\n",
    "ax[1, 0].hist(X_train)\n",
    "ax[1, 1].hist(X_test)\n",
    "ax[0, 0].set_title('Training data')\n",
    "ax[0, 1].set_title('Test data')\n",
    "\n",
    "ax[0, 0].set_ylim(-3, 3)\n",
    "ax[0, 1].set_ylim(-3, 3)\n",
    "ax[1, 0].set_ylim(0, 200)\n",
    "ax[1, 1].set_ylim(0, 200);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa68dde",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-11-11.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e68fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, all_axes = subplots(figsize=[15, 5], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    # Training the model using training data:     \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(new_obs)\n",
    "    y_pred_grid = y_pred.reshape(grid_shape)\n",
    "\n",
    "    # Evaluating the score using test data:\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Scattering the test data only:     \n",
    "    ax.scatter(X_test[:, feature_1], X_test[:, feature_2], c=y_test, s=4, cmap='bwr', marker='.')\n",
    "\n",
    "    ax.contourf(grid_1, grid_2, y_pred_grid, cmap='gray_r', alpha=.2, levels=contour_levels)\n",
    "\n",
    "    ax.set_ylim(ft_min, ft_max)\n",
    "    ax.set_xlim(ft_min, ft_max)\n",
    "    ax.set_yticks([-1.5, 0, 1.5])\n",
    "    ax.set_xticks([-1.5, 0, 1.5])\n",
    "\n",
    "    label = '{} - Score: {:.2f}'.format(name, score)\n",
    "    ax.set_title(label , fontsize=10);\n",
    "    \n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bca055",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-12-13.png\" width=\"1440\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Now the data are mixed in the plane and there is no simple way to separate the two classes. \n",
    "We can see in the plots how the algorithms try to cope with their different strategies. One thing that is immediately obvious is that the fitting patterns are different. Particularly, we can see the fragmented outcome of the _decision tree_ classifier and the smooth elliptic area found by the _support vector classifier (SVC)_ with radial basis functions (RBF) and the neural network (MLP). On a closer look, you may also notice that with ensemble methods in the upper row, the patterns are somewhat disorganised. This is due to the way ensemble methods work: they sample the data randomly and then class them into different categories based on their labels.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "If the prediction was made by chance (throwing a dice), one would expect a 50 % score. Thus, the example also shows that the performance depends on the type of problem and that this testing helps to find an optimal classifier. \n",
    "</p>\n",
    "\n",
    "## **Never expose the test data**\n",
    "<p style='text-align: justify;'>\n",
    "Testing a model on data that is used in training is a methodological mistake. It is therefore vital that the test data is **never, ever** used for training a model at any stage. This is one of the most fundamental principles of machine learning, and its importance cannot be exaggerated. There are numerous examples of people making this mistake one way or another, especially where multiple classification algorithms are used to address a problem.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d029582",
   "metadata": {},
   "source": [
    "## The Stratified Shuffle Split\n",
    "<p style='text-align: justify;'>\n",
    "One potential bias arises when we try to improve the performance of our models through the change of the so-called __hyperparameters__ (instead of using the default parameters as we did so far). We will always receive the optimal output given __the specific test data chosen__. This may lead to overfitting the model on the chosen training and testing data. This can be avoided by choosing different splits into testing and training data and repeating the fit procedure. Doing different splits while preserving the fraction of labels of each class in the original data, the method is called the __stratified shuffle split__.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "We first need to import and instantiate the splitter. We set key word argument `n_splits` to determine the number of different splits. `test_size` lets us determine what fraction of samples is used for the testing data.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0395463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(random_state=RANDOM_STATE, n_splits=10, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d893a",
   "metadata": {},
   "source": [
    "Let us look at the different splits obtained:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cad566",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=[10, 5])\n",
    "\n",
    "n_splits = sss.n_splits\n",
    "split_data_indices = sss.split(X=X, y=y)\n",
    "\n",
    "for index, (tr, tt) in enumerate(split_data_indices):\n",
    "    indices = X[:, feature_1].copy()\n",
    "    indices[tt] = 1\n",
    "    indices[tr] = 0\n",
    "\n",
    "    # Visualize the results\n",
    "    x_axis = arange(indices.size)\n",
    "    y_axis = [index + .5] * indices.size\n",
    "    ax.scatter(x_axis, y_axis, c=indices, marker='_', lw=10, cmap='coolwarm', vmin=-.2, vmax=1.2)\n",
    "\n",
    "# Plot the data classes and groups at the end\n",
    "class_y = [index + 1.5] * indices.size\n",
    "ax.scatter(x_axis, class_y, c=y, marker='_', lw=10, cmap='coolwarm')\n",
    "\n",
    "# Formatting\n",
    "ylabels = list(range(n_splits))\n",
    "ylabels.extend(['Data'])\n",
    "\n",
    "ax.set_yticks(arange(n_splits + 1) + .5)\n",
    "ax.set_yticklabels(ylabels)\n",
    "ax.set_xlabel('Sample index')\n",
    "ax.set_ylabel('SSS iteration');\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32512615",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-14-15.png\" width=\"960\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "By choosing n_splits=10, we obtained ten different splits that have similarly distributed train and test data subsets from the original data. The fraction of the data set aside for testing is 30 %. The different splits cover the whole data set evenly. As such, using them for training and testing will lead to a fairly unbiased average performance. \n",
    "</p>\n",
    "\n",
    "Let us look at the data in state space to check that the classification task is now a real challenge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57232057",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(8, 8))\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    ax.scatter(X[train_index, 0], X[train_index, 1], c=y[train_index], cmap='Set1', s=30, marker='^', alpha=.5)\n",
    "    ax.scatter(X[test_index, 0], X[test_index, 1], c=y[test_index], cmap='cool', s=30, alpha=.5, marker='*', label='Test');\n",
    "    \n",
    "show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7498203",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-15-17.png\" width=\"768\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "These are the scatter plots of the training (magenta) and testing (blue) data. Here are their distributions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(8, 8))\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    ax.hist(X[train_index], color=['magenta', 'red'], alpha=.5, histtype='step')\n",
    "    ax.hist(X[test_index], color=['cyan', 'blue'], alpha=.4, histtype='step');\n",
    "    \n",
    "show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c03537",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-16-19.png\" width=\"768\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The distributions differ in height because less data are in the testing test. Otherwise they are similarly centred and spread. Using a number of realisations (instead of just one) we expect to obtain a more accurate and robust result of the training. \n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "We now train our classifiers on these different splits and obtain the respective scores. They will give a robust measure of the classifier's performance given the data and avoid potential bias due to the selection of specific test data.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b11cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, factor=.3, noise=.4, random_state=RANDOM_STATE)\n",
    "\n",
    "split_data_indices = sss.split(X=X, y=y)\n",
    "\n",
    "score = list()\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_s, y_s = X[train_index, :], y[train_index]\n",
    "    new_obs_s, y_test_s = X[test_index, :], y[test_index]\n",
    "    \n",
    "    score_clf = list()\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        \n",
    "        clf.fit(X_s, y_s)\n",
    "        y_pred = clf.predict(new_obs_s)\n",
    "        score_clf.append(clf.score(new_obs_s, y_test_s))\n",
    "        \n",
    "    score.append(score_clf)\n",
    "    \n",
    "score_mean = mean(score, axis=0)\n",
    "\n",
    "bins = arange(len(score_mean))\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "ax.bar(bins, score_mean);\n",
    "ax.set_xticks(arange(0,8)+0.4)\n",
    "ax.set_xticklabels(classifiers.keys(), rotation=-70);\n",
    "\n",
    "show()\n",
    "\n",
    "print(classifiers.keys())\n",
    "print('Average scores: ')\n",
    "print([\"{0:0.2f}\".format(ind) for ind in score_mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c7482",
   "metadata": {},
   "source": [
    "```{=html}\n",
    "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=5000, random_state=1234)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=5000, random_state=1234)</pre></div></div></div></div></div>\n",
    "```\n",
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-17-21.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "```{.output}\n",
    "dict_keys(['Random Forest', 'AdaBoost (Random Forest)', 'Extra Trees', 'AdaBoost (Extra Tree)', 'Decision Tree', 'SVC (RBF)', 'SVC (Linear)', 'Multi-layer Perceptron'])\n",
    "Average scores: \n",
    "['0.76', '0.76', '0.75', '0.75', '0.70', '0.79', '0.50', '0.78']\n",
    "```\n",
    "\n",
    "The result is the average score for the ten splits performed. All results for the noise-contaminated data are now in the seventies.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This is still good given the quality of the data. It appears that the _decision tree_ classifier gives the lowest result for this kind of problem, _SVC (RBF)_ scores highest. We have to keep in mind, however, that we are using the classifiers with their default settings. We will later use variation of the so-called hyperparameters to further improve the classification score. \n",
    "</p>\n",
    "\n",
    "Here we have used a for loop to train and test on each of the different splits of the data. SciKit Learn also contains functions that take the stratified shuffle split as an argument, e.g. `permutation_test_score`. In that case, the splits do not need to be done separately. \n",
    "\n",
    "We have now reached a point where we can trust to have a robust and unbiased outcome of the training. Let us now look at more refined ways to quantify the result.\n",
    "\n",
    "## Evaluation: ROC and AUC\n",
    "<p style='text-align: justify;'>\n",
    "There are various measures that may be used to evaluate the performance of a machine learning model. Such measures look at different characteristics, including the goodness of fit and generalisability of a model. Evaluation measures used with regards to classification models include, but are not limited to:\n",
    "</p>\n",
    "\n",
    "- Receiver Operation Characteristic (ROC) and Area Under the Curve (AUC) - for binary classifiers.\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "There are many other metrics that, depending on the problem, we may use to evaluate a machine learning model. Please see [the official documentations](https://scikit-learn.org/stable/modules/model_evaluation.html) for additional information on these measures and their implementation in SciKit Learn.\n",
    "</p>\n",
    "\n",
    "The quantities we are going to look at are the __Receiver Operation Characteristic (ROC)__ and the __Area Under the Curve (AUC)__.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "A receiver operation characteristic, often referred to as the __ROC curve__, is a visualisation of the discrimination threshold in a binary classification model. It illustrates the rate of true positives (TPR) against the rate of false positives (FPR) at different thresholds. The aforementioned rates are essentially defined as:\n",
    "</p>\n",
    "\n",
    "- True Positive Rate (TPR): the sensitivity of the model\n",
    "- False Positive Rate (FPR): one minus the specificity of the model\n",
    "\n",
    "This makes ROC a measure of sensitivity versus specificity. \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The area under the ROC curve, often referred to as AUC, reduces the information contained within a ROC curve down to a value between 0 and 1, with 1 being a perfect fit. An AUC value of 0.5 represents any random guess, and values below demonstrate a performance that’s even worse than a lucky guess!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3820c",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "`SciKit Learn` includes specialist functions called `roc_curve` and  `roc_auc_score` to obtain ROC (FPR and TPR values for visualisation) and AUC respectively. Both functions receive as input arguments the test labels (i.e. `y_test`) and the score (probability) associated with each prediction. We obtain the latter measure using one of the following two techniques:\n",
    "</p>\n",
    "\n",
    "- Decision function: where classification models have a `.decision_function` method that provides us with score associated with each label.\n",
    "- Probability: where classification models have a `.predict_proba`  method that provides us with the probability associated with each prediction (we used it in the Classification Introduction lesson). In this case, however, the results are provided in the form of a two-dimensional array where columns represent different labels (as defined in   property). Given that we will plot ROC curves for binary problems (two labels), we only pick one of these columns. Usually, the second column (the feature representing `True` or __1__) is the one to choose. However, if you notice that the results are unexpectedly bad, you may try the other column just be sure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfbc7b",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "We can see that our classifiers now reach different degrees of prediction. The degree can be quantified by the \n",
    "__Area Under the Curve (AUC)__. It refers to the area between the blue ROC curve and the orange diagonal.\n",
    "The area under the ROC curve, often referred to as AUC, reduces the information contained within a ROC curve down to a value between and 0 and 1, with 1 being a perfect fit. An AUC value of 0.5 represents a random guess, and values below the diagonal demonstrate a performance that’s even worse than a guess!\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "SciKit Learn includes specialist functions called `roc_curve` and `roc_auc_score` to obtain ROC (FPR and TPR values for visualisation) and AUC respectively. Both function receive as input arguments the test labels (i.e. y_score) and the score (probability) associated with each prediction. We obtain the latter measure using one of the following two techniques:\n",
    "</p>\n",
    "\n",
    "- Decision function: where classification models have a `.decision_function` method that provides us with a score associated with each label.\n",
    "- Probability: where classification models have a `predict_proba_` method that provides us with the probability associated with each prediction. In this case, however, the results are provided in the form of a two-dimensional array where columns represents different labels (as defined in  `.classes` property). Given that we only plot ROC curves for binary problems, we should only use one of these columns. Usually, the second column (the feature representing `True` or __1__) is the one to choose. However, if you notice that the results are unexpectedly bad, you may try the other column just be sure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fig, all_axes = subplots(figsize=[15, 10], ncols=4, nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "for ax, (name, clf) in zip(all_axes.ravel(), classifiers.items()):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Checking whether or not the object has `decision_function`:\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        # If it does:\n",
    "        y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        # Otherwise:\n",
    "        y_score = clf.predict_proba(X_test)[:, feature_2]  # We only need one column.\n",
    "\n",
    "    # Obtaining the x- and y-axis values for the ROC curve:\n",
    "    fpr, tpr, thresh = roc_curve(y_test, y_score)\n",
    "\n",
    "    # Obtaining the AUC value: \n",
    "    roc_auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2)\n",
    "    ax.plot([0, 1], [0, 1], lw=1, linestyle='--')\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "    label = '{} - AUC: {:.2f}'.format(name, roc_auc)\n",
    "    ax.set_title(label, fontsize=10)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1432e2",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-18-23.png\" width=\"1440\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The (orange) diagonal represents predictions of the two labels by a coin toss. To be of value the classifier must reach a ROC curve above the diagonal. \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This concludes our first steps into classification with SciKit Learn. There are many more aspects of classification. From a practical point of view, [data normalisation](https://scikit-learn.org/stable/modules/preprocessing.html) and [permutation test score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html) as well as the workflow report are important. These will be the topics of our next lesson. \n",
    "</p>\n",
    "\n",
    "<br />\n",
    "\n",
    "### **Lesson Assignment** {#assign}\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "The assignment for this lesson consists of the questions shown below in the next section and can be accessed via GitHub classroom. \n",
    "\n",
    "- For **L2D online cohort**, please click this link: [**GitHub classroom assignment link**](https://classroom.github.com/a/ryp1cKU0)\n",
    "\n",
    "- For **LiDo cohort**, please click this link: [**GitHub classroom assignment link**](https://classroom.github.com/a/Rikq1DAv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53611d53",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "1. You will need to login to your GitHub account.\n",
    "\n",
    "2. Choose your name from the list of students.\n",
    "\n",
    "3. Accept the assignment.\n",
    "\n",
    "4. Refresh the page.\n",
    "\n",
    "By doing these steps, you will be able to access the assignment repository at GitHub. Instructions for completing the assignment are displayed beneath the file browser containing all the files you will need. These instructions are contained in the `README.md` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8d3d0",
   "metadata": {},
   "source": [
    "#### Assignment Questions\n",
    "\n",
    "Take the torus-within-a-torus data generator from the __Challenge__ above. \n",
    "\n",
    "1. Create data with three features and a noise level of 0.3. \n",
    "\n",
    "2. Create a pseudo-3D scatter plot of one of the test data sets to judge the difficulty of the task. \n",
    "\n",
    "3. Train the above introduced classifiers using the stratified shuffle split to generate 10 sets of testing and training data and obtain the average score for each classifier.\n",
    "\n",
    "4. Plot the feature importances obtained from the Random Forest classifier to see the contributions of each feature to the outcome. \n",
    "\n",
    "Note that with 3 or more features it is no longer possible to see the full state space in a plane. \n",
    "\n",
    "5. Optional: Check how the outcome varies depending on\n",
    "\n",
    "    - Choice of seed for random number generator\n",
    "    - Number of data splits\n",
    "    - Percentage of data withheld for testing\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Pick any of the provided (or other) data sets with labels to repeat the above. Feel free to try and do any testing or plotting that you find important. This is not an assignment to get the correct answer. Rather at this stage, we practise to use functionality from SciKit-learn to search for structure in the data that helps to achieve the best predictions possible. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f178d",
   "metadata": {},
   "source": [
    "## Please check these solutions only after submitting the assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3722748",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from numpy import mgrid, linspace, arange, mean, array\n",
    "from numpy.random import uniform, seed\n",
    "\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.pyplot import subplots, axes, scatter, xticks, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c88f66",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_torus_3D(n_samples=100, shuffle=True, noise=None, random_state=None,\n",
    "                 factor=.8):\n",
    "    \"\"\"Make a large torus containing a smaller torus in 3d.\n",
    "\n",
    "    A toy dataset to visualize clustering and classification\n",
    "    algorithms.\n",
    "\n",
    "    Read more in the :ref:`User Guide <sample_generators>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        The total number of points generated. If odd, the inner circle will\n",
    "        have one point more than the outer circle.\n",
    "\n",
    "    shuffle : bool, optional (default=True)\n",
    "        Whether to shuffle the samples.\n",
    "\n",
    "    noise : double or None (default=None)\n",
    "        Standard deviation of Gaussian noise added to the data.\n",
    "\n",
    "    random_state : int, RandomState instance or None (default)\n",
    "        Determines random number generation for dataset shuffling and noise.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    factor : 0 < double < 1 (default=.8)\n",
    "        Scale factor between inner and outer circle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array of shape [n_samples, 2]\n",
    "        The generated samples.\n",
    "\n",
    "    y : array of shape [n_samples]\n",
    "        The integer labels (0 or 1) for class membership of each sample.\n",
    "    \"\"\"\n",
    "    from numpy import pi, linspace, cos, sin, append, ones, zeros, hstack, vstack, intp\n",
    "    from sklearn.utils import check_random_state, shuffle\n",
    "\n",
    "    if factor >= 1 or factor < 0:\n",
    "        raise ValueError(\"'factor' has to be between 0 and 1.\")\n",
    "\n",
    "    n_samples_out = n_samples // 2\n",
    "    n_samples_in = n_samples - n_samples_out\n",
    "\n",
    "    co, ao, ci, ai = 3, 1, 3.6, 0.2\n",
    "    generator = check_random_state(random_state)\n",
    "    # to not have the first point = last point, we set endpoint=False\n",
    "    linspace_out = linspace(0, 2 * pi, n_samples_out, endpoint=False)\n",
    "    linspace_in  = linspace(0, 2 * pi, n_samples_in,  endpoint=False)\n",
    "    outer_circ_x = (co+ao*cos(linspace_out)) * cos(linspace_out*61.1)\n",
    "    outer_circ_y = (co+ao*cos(linspace_out)) * sin(linspace_out*61.1)\n",
    "    outer_circ_z =    ao*sin(linspace_out)\n",
    "    \n",
    "    inner_circ_x = (ci+ai*cos(linspace_in)) * cos(linspace_in*61.1)* factor\n",
    "    inner_circ_y = (ci+ai*cos(linspace_in)) * sin(linspace_in*61.1) * factor\n",
    "    inner_circ_z =    ai*sin(linspace_in) * factor\n",
    "\n",
    "    X = vstack([append(outer_circ_x, inner_circ_x),\n",
    "                append(outer_circ_y, inner_circ_y),\n",
    "                append(outer_circ_z, inner_circ_z)]).T\n",
    "            \n",
    "    y = hstack([zeros(n_samples_out, dtype=intp),\n",
    "                   ones(n_samples_in, dtype=intp)])\n",
    "    \n",
    "    \n",
    "    if shuffle:\n",
    "        X, y = shuffle(X, y, random_state=generator)\n",
    "\n",
    "    if noise is not None:\n",
    "        X += generator.normal(scale=noise, size=X.shape)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5563cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Random Forest)': AdaBoostClassifier(RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE),\n",
    "    'AdaBoost (Extra Tree)': AdaBoostClassifier(ExtraTreesClassifier(random_state=RANDOM_STATE)),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'SVC (RBF)': SVC(random_state=RANDOM_STATE),\n",
    "    'SVC (Linear)': LinearSVC(random_state=RANDOM_STATE),\n",
    "    'Multi-layer Perceptron': MLPClassifier(max_iter=5000, random_state=RANDOM_STATE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee07a2",
   "metadata": {},
   "source": [
    "### Q1 and Q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb376606",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(RANDOM_STATE)\n",
    "\n",
    "X, y = make_torus_3D(n_samples=2000, factor=.5, noise=.3, random_state=RANDOM_STATE)\n",
    "\n",
    "feature_1, feature_2, feature_3 = 0, 1, 2\n",
    "\n",
    "fig, ax = subplots(figsize=(12, 9))\n",
    "ax.set_visible(False)\n",
    "\n",
    "ax = axes(projection=\"3d\")\n",
    "\n",
    "im = ax.scatter3D(X[:, feature_1], X[:, feature_2], X[:, feature_3], \n",
    "                  marker='o', s=20, c=y, cmap='bwr');\n",
    "\n",
    "ax.set_xlabel('Feature A')\n",
    "ax.set_ylabel('Feature B')\n",
    "ax.set_zlabel('Feature C')\n",
    "\n",
    "ax.view_init(30, 50);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5d4ce",
   "metadata": {},
   "source": [
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-22-25.png\" width=\"1152\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(random_state=RANDOM_STATE, n_splits=10, test_size=0.3)\n",
    "\n",
    "split_data_indices = sss.split(X=X, y=y)\n",
    "\n",
    "score = list()\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_s, y_s = X[train_index, :], y[train_index]\n",
    "    new_obs_s, y_test_s = X[test_index, :], y[test_index]\n",
    "    \n",
    "    score_clf = list()\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        \n",
    "        clf.fit(X_s, y_s)\n",
    "        y_pred = clf.predict(new_obs_s)\n",
    "        score_clf.append(clf.score(new_obs_s, y_test_s))\n",
    "        \n",
    "    score.append(score_clf)\n",
    "    \n",
    "score_mean = mean(score, axis=0)\n",
    "\n",
    "bins = arange(len(score_mean))\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "ax.bar(bins, score_mean);\n",
    "\n",
    "show()\n",
    "\n",
    "print(classifiers.keys())\n",
    "print('Average scores: ')\n",
    "print([\"{0:0.2f}\".format(ind) for ind in score_mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606eff9",
   "metadata": {},
   "source": [
    "```{=html}\n",
    "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=5000, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=5000, random_state=123)</pre></div></div></div></div></div>\n",
    "```\n",
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-23-27.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "```{.output}\n",
    "dict_keys(['Random Forest', 'AdaBoost (Random Forest)', 'Extra Trees', 'AdaBoost (Extra Tree)', 'Decision Tree', 'SVC (RBF)', 'SVC (Linear)', 'Multi-layer Perceptron'])\n",
    "Average scores: \n",
    "['0.87', '0.88', '0.87', '0.87', '0.83', '0.89', '0.49', '0.88']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "clf_RF.fit(X_s, y_s)\n",
    "\n",
    "y_pred = clf_RF.predict(new_obs_s)\n",
    "\n",
    "score_RF = clf_RF.score(new_obs_s, y_test_s)\n",
    "\n",
    "print('Random Forest score:', score_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab23bc9",
   "metadata": {},
   "source": [
    "```{=html}\n",
    "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=123)</pre></div></div></div></div></div>\n",
    "```\n",
    "\n",
    "```{.output}\n",
    "Random Forest score: 0.88\n",
    "```\n",
    "\n",
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf_RF.feature_importances_\n",
    "\n",
    "template = 'Feature 1: {:.1f}%; Feature 2: {:.1f}%; Feature 3: {:.1f}%' \n",
    "\n",
    "print(template.format(importances[0]*100, importances[1]*100, importances[2]*100))\n",
    "\n",
    "bins = arange(importances.shape[0])\n",
    "\n",
    "fig, ax = subplots()\n",
    "\n",
    "ax.bar(bins, importances, color=('g', 'm', 'b'));\n",
    "ax.set_ylabel('Feature Importance', fontsize=16)\n",
    "\n",
    "xticks(bins, ('Feature 1', 'Feature 2', 'Feature 3'), fontsize=16);\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68298c4d",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "Feature 1: 31.4%; Feature 2: 33.9%; Feature 3: 34.7%\n",
    "```\n",
    "\n",
    "<img src=\"fig/02-improvement-rendered-unnamed-chunk-25-29.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The three features contribute similarly to the outcome.\n",
    "\n",
    "<br />\n",
    "\n",
    "### **Forum for Questions** {#forum}\n",
    "\n",
    "----------------------\n",
    "Any questions related to this lesson, code or assignment can be posted at our forum (**Students_team**) on GitHub.\n",
    "\n",
    "- For **L2D online cohort**, the forum can be accessed at: [**Students_team**](https://github.com/orgs/L2D-Oct2022/teams/students_team)\n",
    "\n",
    "- For **LiDo cohort**, the forum can be accessed at: [**LiDo_students_team**](https://github.com/orgs/LIDo-2022/teams/lido_students_team)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be445742",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "The forum can only be accessed via your GitHub account once you have accepted the invitation to be included in the team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2586800",
   "metadata": {},
   "source": [
    "- Different classification algorithms approach problems differently.\n",
    "- `train_test_split` function tries to preserve the ratio of labels in the split\n",
    "- Increasing the level of noise in the data makes the task more complicated.\n",
    "- The potential bias due to splitting could be avoid using stratified shuffle split.\n",
    "- `StratifiedShuffleSplit` is a method that uses `n_splits` and `test_size` parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d7fd9",
   "metadata": {},
   "source": [
    "[r-markdown]: https://rmarkdown.rstudio.com/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
